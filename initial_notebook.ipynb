{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"},{"sourceId":6060,"sourceType":"modelInstanceVersion","modelInstanceId":4681,"modelId":2819}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Natural language inference\n\n### Contradictory dear watson\n***\n\nProject: https://www.kaggle.com/competitions/contradictory-my-dear-watson\n\n__Primer approach al problema de NLI__\n\nEl objetivo de este notebook es explorar el dataset y testear distintos enfoques al problema: \n    \n- Hipothesis Baseline Approach\n- Classic Machine learning \n- Neural Networks\n\n\nDatasets: \n- Train\n- Test\n- Submission\n\nLabels: \n\n- entailment = 0 \n- neutral = 1\n- contradiction = 2\n\nModelos a comparar: \n\n- bert_base_multi\n\n- deberta_v3_base_multi\n\n- distil_bert_base_multi\n\n- xlm_roberta_base_multi\n\n- xlm_roberta_large_multi\n\n__Modelo Bert Keras__\n\nhttps://keras.io/api/keras_nlp/models/bert/\n\n\nReferencias: \n\nhttps://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-contradictory-dearwatson\n","metadata":{}},{"cell_type":"markdown","source":"__librerias a instalar__","metadata":{}},{"cell_type":"code","source":"#!pip install -q keras-nlp --upgrade\n#!pip install seaborn\n#pip install tensorflow\n#pip install tensorflow-text","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:01:20.944147Z","iopub.execute_input":"2024-09-05T22:01:20.945164Z","iopub.status.idle":"2024-09-05T22:01:20.952294Z","shell.execute_reply.started":"2024-09-05T22:01:20.945111Z","shell.execute_reply":"2024-09-05T22:01:20.951389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__importar librerias__","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_nlp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:01:20.953852Z","iopub.execute_input":"2024-09-05T22:01:20.954720Z","iopub.status.idle":"2024-09-05T22:01:40.338440Z","shell.execute_reply.started":"2024-09-05T22:01:20.954667Z","shell.execute_reply":"2024-09-05T22:01:40.337645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Conf__","metadata":{}},{"cell_type":"code","source":"try:\n    # detect and init the TPU\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()  # default strategy if no TPU available","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:02:40.078513Z","iopub.execute_input":"2024-09-05T22:02:40.079367Z","iopub.status.idle":"2024-09-05T22:02:48.809613Z","shell.execute_reply.started":"2024-09-05T22:02:40.079332Z","shell.execute_reply":"2024-09-05T22:02:48.808846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RESULT_DICT = {\n    0 : \"entailment\",\n    1 : \"neutral\",\n    2 : \"contradiction\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:02:54.467680Z","iopub.execute_input":"2024-09-05T22:02:54.468022Z","iopub.status.idle":"2024-09-05T22:02:54.471987Z","shell.execute_reply.started":"2024-09-05T22:02:54.467994Z","shell.execute_reply":"2024-09-05T22:02:54.471153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__data__","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:16.469324Z","iopub.execute_input":"2024-09-05T22:04:16.470156Z","iopub.status.idle":"2024-09-05T22:04:16.621639Z","shell.execute_reply.started":"2024-09-05T22:04:16.470123Z","shell.execute_reply":"2024-09-05T22:04:16.620880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:17.849312Z","iopub.execute_input":"2024-09-05T22:04:17.849617Z","iopub.status.idle":"2024-09-05T22:04:17.865422Z","shell.execute_reply.started":"2024-09-05T22:04:17.849592Z","shell.execute_reply":"2024-09-05T22:04:17.864557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_pair_of_sentence(x):\n    print( \"Premise : \" + x['premise'])\n    print( \"Hypothesis: \" + x['hypothesis'])\n    print( \"Language: \" + x['language'])\n    print( \"Label: \" + str(x['label']))\n    print()\n\ndf_train.head(5).apply(lambda x : display_pair_of_sentence(x), axis=1)\n\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:24.639072Z","iopub.execute_input":"2024-09-05T22:04:24.639449Z","iopub.status.idle":"2024-09-05T22:04:24.649160Z","shell.execute_reply.started":"2024-09-05T22:04:24.639417Z","shell.execute_reply":"2024-09-05T22:04:24.648054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4))\n\nsns.set_color_codes(\"pastel\")\nsns.despine()\nax = sns.countplot(data=df_train, \n                   y=\"label\",\n                   order = df_train['label'].value_counts().index)\n\nabs_values = df_train['label'].value_counts(ascending=False)\nrel_values = df_train['label'].value_counts(ascending=False, normalize=True).values * 100\nlbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n\nax.bar_label(container=ax.containers[0], labels=lbls)\n\nax.set_yticklabels([RESULT_DICT[index] for index in abs_values.index])\n\nax.set_title(\"Distribution of labels in the training set\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:25.482461Z","iopub.execute_input":"2024-09-05T22:04:25.483303Z","iopub.status.idle":"2024-09-05T22:04:25.690366Z","shell.execute_reply.started":"2024-09-05T22:04:25.483268Z","shell.execute_reply":"2024-09-05T22:04:25.689371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 10))\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.despine()\nax = sns.countplot(data=df_train, \n                   y=\"language\",\n                   order = df_train['language'].value_counts().index)\n\nabs_values = df_train['language'].value_counts(ascending=False)\nrel_values = df_train['language'].value_counts(ascending=False, normalize=True).values * 100\nlbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(abs_values, rel_values)]\n\nax.bar_label(container=ax.containers[0], labels=lbls)\n\nax.set_title(\"Distribution of languages in the training set\")","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:26.561397Z","iopub.execute_input":"2024-09-05T22:04:26.561744Z","iopub.status.idle":"2024-09-05T22:04:26.896109Z","shell.execute_reply.started":"2024-09-05T22:04:26.561715Z","shell.execute_reply":"2024-09-05T22:04:26.895112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\ndf_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\ndf_train[[\"hypothesis_length\", \"premise_length\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:29.646524Z","iopub.execute_input":"2024-09-05T22:04:29.647323Z","iopub.status.idle":"2024-09-05T22:04:29.672836Z","shell.execute_reply.started":"2024-09-05T22:04:29.647290Z","shell.execute_reply":"2024-09-05T22:04:29.671986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Data preprocessing__\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n\nBert is only trained in English corpus. That's why people use multilingual Bert or XLM-Roberta for this competition.\n\nHere are some models for multi-language NLP available in Keras NLP:\n\nbert_base_multi\n\ndeberta_v3_base_multi\n\ndistil_bert_base_multi\n\nxlm_roberta_base_multi\n\nxlm_roberta_large_multi","metadata":{}},{"cell_type":"code","source":"VALIDATION_SPLIT = 0.2\nTRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:33.624249Z","iopub.execute_input":"2024-09-05T22:04:33.625087Z","iopub.status.idle":"2024-09-05T22:04:33.629238Z","shell.execute_reply.started":"2024-09-05T22:04:33.625046Z","shell.execute_reply":"2024-09-05T22:04:33.628253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a utility function that splits the example into an (x, y) tuple that is suitable for model.fit().\n\nBy default, keras_nlp.models.BertClassifier will tokenize and pack together raw strings using a \"[SEP]\" token during training.\n\nTherefore, this label splitting is all the data preparation that we need to perform.","metadata":{}},{"cell_type":"code","source":"def split_labels(x, y):\n    return (x[0], x[1]), y\n\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            df_train[['premise','hypothesis']].values,\n            keras.utils.to_categorical(df_train['label'], num_classes=3)\n        )\n    )\n)\n\ntrain_dataset = training_dataset.take(TRAIN_SIZE)\nval_dataset = training_dataset.skip(TRAIN_SIZE)\n\n# Apply the preprocessor to every sample of train, val and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\ntrain_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\nval_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:36.216219Z","iopub.execute_input":"2024-09-05T22:04:36.216540Z","iopub.status.idle":"2024-09-05T22:04:36.304192Z","shell.execute_reply.started":"2024-09-05T22:04:36.216514Z","shell.execute_reply":"2024-09-05T22:04:36.303327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a BERT model.\nwith strategy.scope():\n    classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_base_multi\", num_classes=3)\n\n    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                       metrics=['accuracy'])\n    \n    classifier.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:04:38.391642Z","iopub.execute_input":"2024-09-05T22:04:38.391963Z","iopub.status.idle":"2024-09-05T22:05:20.269938Z","shell.execute_reply.started":"2024-09-05T22:04:38.391936Z","shell.execute_reply":"2024-09-05T22:05:20.268870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Fine tunning BERT__","metadata":{}},{"cell_type":"code","source":"EPOCHS=30\nhistory = classifier.fit(train_preprocessed,\n                         epochs=EPOCHS,\n                         validation_data=val_preprocessed\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:05:20.271464Z","iopub.execute_input":"2024-09-05T22:05:20.272231Z","iopub.status.idle":"2024-09-05T22:22:35.439653Z","shell.execute_reply.started":"2024-09-05T22:05:20.272198Z","shell.execute_reply":"2024-09-05T22:22:35.438599Z"},"trusted":true},"execution_count":null,"outputs":[]}]}