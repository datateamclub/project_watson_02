{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceType":"competition","sourceId":21733,"databundleVersionId":1408234},{"sourceType":"modelInstanceVersion","sourceId":6070,"databundleVersionId":7429252,"modelInstanceId":4691}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Natural language inference\n\n### Contradictory dear watson\n***\n\nProject: https://www.kaggle.com/competitions/contradictory-my-dear-watson\n\n### Model: bert fine tunning\n\nhttps://huggingface.co/distilbert/distilbert-base-multilingual-cased","metadata":{}},{"cell_type":"markdown","source":"__Objetivo__: testear el modelo entendiendo su funcionamiento, bias y limitaciones","metadata":{"vscode":{"languageId":"plaintext"}}},{"cell_type":"markdown","source":"This model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.\n\nThe model is trained on the concatenation of Wikipedia in 104 different languages listed here. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base). On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base","metadata":{"vscode":{"languageId":"plaintext"}}},{"cell_type":"markdown","source":"**Importar Librerias**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_nlp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:15.489805Z","iopub.execute_input":"2024-09-12T22:56:15.490333Z","iopub.status.idle":"2024-09-12T22:56:15.495387Z","shell.execute_reply.started":"2024-09-12T22:56:15.490296Z","shell.execute_reply":"2024-09-12T22:56:15.494559Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.16.1\nKerasNLP version: 0.14.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"__Conf__","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:47:04.655602Z","iopub.execute_input":"2024-09-05T22:47:04.656333Z","iopub.status.idle":"2024-09-05T22:47:05.530781Z","shell.execute_reply.started":"2024-09-05T22:47:04.65629Z","shell.execute_reply":"2024-09-05T22:47:05.529252Z"}}},{"cell_type":"code","source":"try:\n    # detect and init the TPU\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()  # default strategy if no TPU available","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:15.496643Z","iopub.execute_input":"2024-09-12T22:56:15.497027Z","iopub.status.idle":"2024-09-12T22:56:23.438564Z","shell.execute_reply.started":"2024-09-12T22:56:15.496982Z","shell.execute_reply":"2024-09-12T22:56:23.437213Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1726181778.442075    1495 service.cc:145] XLA service 0x5b6d95e55a20 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1726181778.442135    1495 service.cc:153]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1726181778.442139    1495 service.cc:153]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1726181778.442142    1495 service.cc:153]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1726181778.442145    1495 service.cc:153]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1726181778.442148    1495 service.cc:153]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1726181778.442151    1495 service.cc:153]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1726181778.442154    1495 service.cc:153]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1726181778.442157    1495 service.cc:153]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nAll devices:  [LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]\n","output_type":"stream"}]},{"cell_type":"code","source":"RESULT_DICT = {\n    0 : \"entailment\",\n    1 : \"neutral\",\n    2 : \"contradiction\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:23.441143Z","iopub.execute_input":"2024-09-12T22:56:23.441966Z","iopub.status.idle":"2024-09-12T22:56:23.446281Z","shell.execute_reply.started":"2024-09-12T22:56:23.441908Z","shell.execute_reply":"2024-09-12T22:56:23.445262Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:23.447526Z","iopub.execute_input":"2024-09-12T22:56:23.447867Z","iopub.status.idle":"2024-09-12T22:56:23.542329Z","shell.execute_reply.started":"2024-09-12T22:56:23.447828Z","shell.execute_reply":"2024-09-12T22:56:23.541422Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def display_pair_of_sentence(x):\n    print( \"Premise : \" + x['premise'])\n    print( \"Hypothesis: \" + x['hypothesis'])\n    print( \"Language: \" + x['language'])\n    print( \"Label: \" + str(x['label']))\n    print()\n\ndf_train.head(5).apply(lambda x : display_pair_of_sentence(x), axis=1)\n\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:28.146085Z","iopub.execute_input":"2024-09-12T22:56:28.146399Z","iopub.status.idle":"2024-09-12T22:56:28.157201Z","shell.execute_reply.started":"2024-09-12T22:56:28.146372Z","shell.execute_reply":"2024-09-12T22:56:28.156199Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Premise : and these comments were considered in formulating the interim rules.\nHypothesis: The rules developed in the interim were put together with these comments in mind.\nLanguage: English\nLabel: 0\n\nPremise : These are issues that we wrestle with in practice groups of law firms, she said. \nHypothesis: Practice groups are not permitted to work on these issues.\nLanguage: English\nLabel: 2\n\nPremise : Des petites choses comme celles-là font une différence énorme dans ce que j'essaye de faire.\nHypothesis: J'essayais d'accomplir quelque chose.\nLanguage: French\nLabel: 0\n\nPremise : you know they can't really defend themselves like somebody grown uh say my age you know yeah\nHypothesis: They can't defend themselves because of their age.\nLanguage: English\nLabel: 0\n\nPremise : ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสดงออกและได้เล่นหลายบทบาทไปพร้อมกัน ๆ อาจช่วยให้เด็กจับความคล้ายคลึงและความแตกต่างระหว่างผู้คนในด้านความปรารถนา ความเชื่อ และความรู้สึกได้\nHypothesis: เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร\nLanguage: Thai\nLabel: 1\n\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(12120, 6)"},"metadata":{}}]},{"cell_type":"code","source":"df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\ndf_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\ndf_train[[\"hypothesis_length\", \"premise_length\"]].describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:28.902849Z","iopub.execute_input":"2024-09-12T22:56:28.903932Z","iopub.status.idle":"2024-09-12T22:56:28.941276Z","shell.execute_reply.started":"2024-09-12T22:56:28.903870Z","shell.execute_reply":"2024-09-12T22:56:28.940373Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       hypothesis_length  premise_length\ncount       12120.000000    12120.000000\nmean           53.892327      107.373185\nstd            25.302358       71.089954\nmin             4.000000        4.000000\n25%            36.000000       55.000000\n50%            51.000000       96.000000\n75%            67.000000      146.000000\nmax           276.000000      967.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hypothesis_length</th>\n      <th>premise_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>12120.000000</td>\n      <td>12120.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>53.892327</td>\n      <td>107.373185</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>25.302358</td>\n      <td>71.089954</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>36.000000</td>\n      <td>55.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>51.000000</td>\n      <td>96.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>67.000000</td>\n      <td>146.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>276.000000</td>\n      <td>967.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Data preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:35:23.362848Z","iopub.execute_input":"2024-09-12T22:35:23.363252Z","iopub.status.idle":"2024-09-12T22:35:23.367303Z","shell.execute_reply.started":"2024-09-12T22:35:23.363217Z","shell.execute_reply":"2024-09-12T22:35:23.366537Z"}}},{"cell_type":"code","source":"df_train = df_train[['premise', 'hypothesis', 'label']]\ndf_test = df_test[['premise', 'hypothesis']]","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:30.525124Z","iopub.execute_input":"2024-09-12T22:56:30.525751Z","iopub.status.idle":"2024-09-12T22:56:30.532396Z","shell.execute_reply.started":"2024-09-12T22:56:30.525720Z","shell.execute_reply":"2024-09-12T22:56:30.531537Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Train model","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:34.721154Z","iopub.execute_input":"2024-09-12T22:56:34.721514Z","iopub.status.idle":"2024-09-12T22:56:37.287889Z","shell.execute_reply.started":"2024-09-12T22:56:34.721469Z","shell.execute_reply":"2024-09-12T22:56:37.287050Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_name ='joeddav/xlm-roberta-large-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:56:39.340063Z","iopub.execute_input":"2024-09-12T22:56:39.340998Z","iopub.status.idle":"2024-09-12T22:56:42.250734Z","shell.execute_reply.started":"2024-09-12T22:56:39.340959Z","shell.execute_reply":"2024-09-12T22:56:42.249792Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Tokenize","metadata":{}},{"cell_type":"code","source":"def encode_premise_sentence(s):\n    tokens=[]\n    tokens.append('[CLS]')\n    tokens+=list(tokenizer.tokenize(s))\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:58:42.626800Z","iopub.execute_input":"2024-09-12T22:58:42.627129Z","iopub.status.idle":"2024-09-12T22:58:42.631708Z","shell.execute_reply.started":"2024-09-12T22:58:42.627102Z","shell.execute_reply":"2024-09-12T22:58:42.630830Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def encode_hypo_sentence(s):\n    tokens=[]\n    tokens.append('[sep]')\n    tokens+=list(tokenizer.tokenize(s))\n    tokens.append('[sep]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:59:16.716642Z","iopub.execute_input":"2024-09-12T22:59:16.717228Z","iopub.status.idle":"2024-09-12T22:59:16.721184Z","shell.execute_reply.started":"2024-09-12T22:59:16.717198Z","shell.execute_reply":"2024-09-12T22:59:16.720267Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"encode_hypo_sentence(\"jsalkgfad\")","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:59:28.404036Z","iopub.execute_input":"2024-09-12T22:59:28.404430Z","iopub.status.idle":"2024-09-12T22:59:28.410063Z","shell.execute_reply.started":"2024-09-12T22:59:28.404398Z","shell.execute_reply":"2024-09-12T22:59:28.409201Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[3, 1647, 2317, 8517, 1021, 71, 3]"},"metadata":{}}]},{"cell_type":"code","source":"encode_premise_sentence(\"jsalkgfad\")","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:59:05.713237Z","iopub.execute_input":"2024-09-12T22:59:05.713763Z","iopub.status.idle":"2024-09-12T22:59:05.726409Z","shell.execute_reply.started":"2024-09-12T22:59:05.713732Z","shell.execute_reply":"2024-09-12T22:59:05.725439Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[3, 1647, 2317, 8517, 1021, 71]"},"metadata":{}}]},{"cell_type":"code","source":"tokenized=[]\nfor i in range(len(df_train)):\n    pre=encode_premise_sentence(df_train['premise'][i])\n    hyp=encode_hypo_sentence(df_train['hypothesis'][i])\n    tokenized.append(pre+hyp)\ndf_train['tokenized']=tokenized\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:00:56.331793Z","iopub.execute_input":"2024-09-12T23:00:56.332165Z","iopub.status.idle":"2024-09-12T23:01:00.229965Z","shell.execute_reply.started":"2024-09-12T23:00:56.332134Z","shell.execute_reply":"2024-09-12T23:01:00.229189Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                             premise  \\\n0  and these comments were considered in formulat...   \n1  These are issues that we wrestle with in pract...   \n2  Des petites choses comme celles-là font une di...   \n3  you know they can't really defend themselves l...   \n4  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis  label  \\\n0  The rules developed in the interim were put to...      0   \n1  Practice groups are not permitted to work on t...      2   \n2              J'essayais d'accomplir quelque chose.      0   \n3  They can't defend themselves because of their ...      0   \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร      1   \n\n                                           tokenized  \n0  [3, 136, 6097, 24626, 3542, 90698, 23, 26168, ...  \n1  [3, 32255, 621, 37348, 450, 642, 148, 56644, 1...  \n2  [3, 5581, 69332, 37899, 3739, 91362, 9, 16161,...  \n3  [3, 398, 3714, 1836, 831, 25, 18, 6183, 65922,...  \n4  [3, 6976, 114538, 171936, 18379, 101830, 14435...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>0</td>\n      <td>[3, 136, 6097, 24626, 3542, 90698, 23, 26168, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>2</td>\n      <td>[3, 32255, 621, 37348, 450, 642, 148, 56644, 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>0</td>\n      <td>[3, 5581, 69332, 37899, 3739, 91362, 9, 16161,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>0</td>\n      <td>[3, 398, 3714, 1836, 831, 25, 18, 6183, 65922,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>1</td>\n      <td>[3, 6976, 114538, 171936, 18379, 101830, 14435...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Attention Mask and Token Type ID","metadata":{}},{"cell_type":"code","source":"mask=[]\nfor i in range(len(df_train)):\n    padded_seq=tokenizer(df_train['premise'][i],df_train['hypothesis'][i], padding=True,add_special_tokens = True)\n    mask.append(padded_seq)\ndf_train['masked'] = mask\ndf_train.head(5)\n# print(mask[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:02:17.923530Z","iopub.execute_input":"2024-09-12T23:02:17.923918Z","iopub.status.idle":"2024-09-12T23:02:21.340518Z","shell.execute_reply.started":"2024-09-12T23:02:17.923889Z","shell.execute_reply":"2024-09-12T23:02:21.339677Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                             premise  \\\n0  and these comments were considered in formulat...   \n1  These are issues that we wrestle with in pract...   \n2  Des petites choses comme celles-là font une di...   \n3  you know they can't really defend themselves l...   \n4  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis  label  \\\n0  The rules developed in the interim were put to...      0   \n1  Practice groups are not permitted to work on t...      2   \n2              J'essayais d'accomplir quelque chose.      0   \n3  They can't defend themselves because of their ...      0   \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร      1   \n\n                                           tokenized  \\\n0  [3, 136, 6097, 24626, 3542, 90698, 23, 26168, ...   \n1  [3, 32255, 621, 37348, 450, 642, 148, 56644, 1...   \n2  [3, 5581, 69332, 37899, 3739, 91362, 9, 16161,...   \n3  [3, 398, 3714, 1836, 831, 25, 18, 6183, 65922,...   \n4  [3, 6976, 114538, 171936, 18379, 101830, 14435...   \n\n                        masked  \n0  [input_ids, attention_mask]  \n1  [input_ids, attention_mask]  \n2  [input_ids, attention_mask]  \n3  [input_ids, attention_mask]  \n4  [input_ids, attention_mask]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>tokenized</th>\n      <th>masked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>0</td>\n      <td>[3, 136, 6097, 24626, 3542, 90698, 23, 26168, ...</td>\n      <td>[input_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>2</td>\n      <td>[3, 32255, 621, 37348, 450, 642, 148, 56644, 1...</td>\n      <td>[input_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>0</td>\n      <td>[3, 5581, 69332, 37899, 3739, 91362, 9, 16161,...</td>\n      <td>[input_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>0</td>\n      <td>[3, 398, 3714, 1836, 831, 25, 18, 6183, 65922,...</td>\n      <td>[input_ids, attention_mask]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>1</td>\n      <td>[3, 6976, 114538, 171936, 18379, 101830, 14435...</td>\n      <td>[input_ids, attention_mask]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Train Model","metadata":{}},{"cell_type":"code","source":"max_len=237\ndef build_model():\n    bert_encoder = TFAutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    embedding = bert_encoder([input_word_ids, input_mask])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:03:41.238518Z","iopub.execute_input":"2024-09-12T23:03:41.238921Z","iopub.status.idle":"2024-09-12T23:03:41.245133Z","shell.execute_reply.started":"2024-09-12T23:03:41.238886Z","shell.execute_reply":"2024-09-12T23:03:41.244341Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def input_convert(data):\n    inputs={\n        'input_word_ids':[],\n        'input_mask':[]\n    }\n    for each in data:\n        inputs['input_word_ids'].append(each['input_ids'])\n        inputs['input_mask'].append(each['attention_mask'])\n        \n    inputs['input_word_ids']= tf.ragged.constant( inputs['input_word_ids']).to_tensor()\n    inputs['input_mask']= tf.ragged.constant( inputs['input_mask']).to_tensor()\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:03:55.488303Z","iopub.execute_input":"2024-09-12T23:03:55.488695Z","iopub.status.idle":"2024-09-12T23:03:55.493700Z","shell.execute_reply.started":"2024-09-12T23:03:55.488660Z","shell.execute_reply":"2024-09-12T23:03:55.492893Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_input=input_convert(df_train['masked'].values)\nfor key in train_input.keys():\n    train_input[key] = train_input[key][:,:max_len]","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:04:16.462597Z","iopub.execute_input":"2024-09-12T23:04:16.462952Z","iopub.status.idle":"2024-09-12T23:04:18.379780Z","shell.execute_reply.started":"2024-09-12T23:04:16.462920Z","shell.execute_reply":"2024-09-12T23:04:18.378774Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_input","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:04:23.487091Z","iopub.execute_input":"2024-09-12T23:04:23.487436Z","iopub.status.idle":"2024-09-12T23:04:23.493624Z","shell.execute_reply.started":"2024-09-12T23:04:23.487405Z","shell.execute_reply":"2024-09-12T23:04:23.492804Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'input_word_ids': <tf.Tensor: shape=(12120, 237), dtype=int32, numpy=\n array([[    0,   136,  6097, ...,     0,     0,     0],\n        [    0, 32255,   621, ...,     0,     0,     0],\n        [    0,  5581, 69332, ...,     0,     0,     0],\n        ...,\n        [    0,   581,  5526, ...,     0,     0,     0],\n        [    0,  1913,    70, ...,     0,     0,     0],\n        [    0,  1326, 66570, ...,     0,     0,     0]], dtype=int32)>,\n 'input_mask': <tf.Tensor: shape=(12120, 237), dtype=int32, numpy=\n array([[1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0],\n        ...,\n        [1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0],\n        [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TFXLMRobertaModel","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:09:18.164171Z","iopub.execute_input":"2024-09-12T23:09:18.164500Z","iopub.status.idle":"2024-09-12T23:09:18.168265Z","shell.execute_reply.started":"2024-09-12T23:09:18.164459Z","shell.execute_reply":"2024-09-12T23:09:18.167407Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Forward pass through the model\n    # Ensure inputs are handled correctly\n    encoder_outputs = bert_encoder(\n        input_ids=input_word_ids,\n        attention_mask=input_mask\n    )\n    embedding = encoder_outputs.last_hidden_state  # Use last_hidden_state for embeddings\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:13:10.418296Z","iopub.execute_input":"2024-09-12T23:13:10.418637Z","iopub.status.idle":"2024-09-12T23:13:10.425282Z","shell.execute_reply.started":"2024-09-12T23:13:10.418609Z","shell.execute_reply":"2024-09-12T23:13:10.424513Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Use tf.function to convert Keras tensors to TensorFlow tensors\n    @tf.function\n    def get_model_outputs(input_ids, attention_mask):\n        return bert_encoder(input_ids=input_ids, attention_mask=attention_mask)\n\n    # Forward pass through the model\n    encoder_outputs = get_model_outputs(input_word_ids, input_mask)\n    embedding = encoder_outputs.last_hidden_state  # Use last_hidden_state for embeddings\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:16:20.886715Z","iopub.execute_input":"2024-09-12T23:16:20.887335Z","iopub.status.idle":"2024-09-12T23:16:20.893716Z","shell.execute_reply.started":"2024-09-12T23:16:20.887302Z","shell.execute_reply":"2024-09-12T23:16:20.892894Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":" import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust this as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Define a function to call the model\n    def call_transformer_model(input_ids, attention_mask):\n        # Ensure the inputs are TensorFlow tensors\n        return bert_encoder(input_ids=input_ids, attention_mask=attention_mask)\n    \n    # Call the model\n    encoder_outputs = call_transformer_model(input_word_ids, input_mask)\n    embedding = encoder_outputs.last_hidden_state  # Get the last hidden state\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])  # Using the first token (CLS token) for classification\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:18:31.140693Z","iopub.execute_input":"2024-09-12T23:18:31.141129Z","iopub.status.idle":"2024-09-12T23:18:31.150092Z","shell.execute_reply.started":"2024-09-12T23:18:31.141093Z","shell.execute_reply":"2024-09-12T23:18:31.148890Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Example of preparing data\ntrain_input = {\n    'input_word_ids': tf.convert_to_tensor(train_enc['input_ids'], dtype=tf.int32),\n    'input_mask': tf.convert_to_tensor(train_enc['attention_mask'], dtype=tf.int32)\n}\n\n# Example of model training\nmodel = build_model()\nmodel.summary()\nmodel.fit(train_input, df_train['label'].values, epochs=5, verbose=1, batch_size=128, validation_split=0.1, callbacks=[early_stop])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:19:25.757146Z","iopub.execute_input":"2024-09-12T23:19:25.757693Z","iopub.status.idle":"2024-09-12T23:19:25.790163Z","shell.execute_reply.started":"2024-09-12T23:19:25.757661Z","shell.execute_reply":"2024-09-12T23:19:25.788988Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of preparing data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_word_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[43mtrain_enc\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(train_enc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example of model training\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model()\n","\u001b[0;31mNameError\u001b[0m: name 'train_enc' is not defined"],"ename":"NameError","evalue":"name 'train_enc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    model.fit(train_input, df_train['label'].values, epochs=5, verbose=1, batch_size=128, validation_split=0.1, callbacks=[early_stop])","metadata":{"execution":{"iopub.status.busy":"2024-09-12T23:18:32.166385Z","iopub.execute_input":"2024-09-12T23:18:32.166783Z","iopub.status.idle":"2024-09-12T23:19:20.373277Z","shell.execute_reply.started":"2024-09-12T23:18:32.166751Z","shell.execute_reply":"2024-09-12T23:19:20.371940Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing TFXLMRobertaModel: ['classifier']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at joeddav/xlm-roberta-large-xnli.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(train_input, df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stop])\n","Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bert_encoder(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Call the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_word_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m embedding \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state  \u001b[38;5;66;03m# Get the last hidden state\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Define the output layer\u001b[39;00m\n","Cell \u001b[0;32mIn[33], line 17\u001b[0m, in \u001b[0;36mbuild_model.<locals>.call_transformer_model\u001b[0;34m(input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_transformer_model\u001b[39m(input_ids, attention_mask):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Ensure the inputs are TensorFlow tensors\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbert_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:513\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(main_input, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(main_input):\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;66;03m# EagerTensors don't allow to use the .name property so we check for a real Tensor\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tfxlm_roberta_model_5' (type TFXLMRobertaModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tfxlm_roberta_model_5' (type TFXLMRobertaModel):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_word_ids>\n  • attention_mask=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"],"ename":"ValueError","evalue":"Exception encountered when calling layer 'tfxlm_roberta_model_5' (type TFXLMRobertaModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tfxlm_roberta_model_5' (type TFXLMRobertaModel):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_word_ids>\n  • attention_mask=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False","output_type":"error"}]},{"cell_type":"markdown","source":"#### Prediction","metadata":{}},{"cell_type":"code","source":"mask=[]\nfor i in range(len(df_test)):\n    padded_seq=tokenizer(df_test['premise'][i],df_test['hypothesis'][i],\n                        padding=True,add_special_tokens =True)\n    mask.append(padded_seq)\ndf_test['masked']=mask\ndf_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=[np.argmax(i) for i in model.predict(test_input)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALIDATION_SPLIT = 0.2\nTRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_labels(x, y):\n    return (x[0], x[1]), y\n\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            df_train[['premise','hypothesis']].values,\n            keras.utils.to_categorical(df_train['label'], num_classes=3)\n        )\n    )\n)\n\ntrain_dataset = training_dataset.take(TRAIN_SIZE)\nval_dataset = training_dataset.skip(TRAIN_SIZE)\n\n# Apply the preprocessor to every sample of train, val and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\ntrain_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\nval_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a BERT model.\nwith strategy.scope():\n    classifier = keras_nlp.models.DistilBertClassifier.from_preset(\"distil_bert_base_multi\", num_classes=3)\n\n    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                       metrics=['accuracy'])\n    \n    classifier.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Fine tunning distil bert__","metadata":{}},{"cell_type":"code","source":"EPOCHS=10\nhistory = classifier.fit(train_preprocessed,\n                         epochs=EPOCHS,\n                         validation_data=val_preprocessed\n                        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}