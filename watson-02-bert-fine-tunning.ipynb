{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"},{"sourceId":6070,"sourceType":"modelInstanceVersion","modelInstanceId":4691,"modelId":2821}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Natural language inference\n\n### Contradictory dear watson\n***\n\nProject: https://www.kaggle.com/competitions/contradictory-my-dear-watson\n\n### Model: bert fine tunning\n\nhttps://huggingface.co/distilbert/distilbert-base-multilingual-cased","metadata":{}},{"cell_type":"markdown","source":"__Objetivo__: testear el modelo entendiendo su funcionamiento, bias y limitaciones","metadata":{"vscode":{"languageId":"plaintext"}}},{"cell_type":"markdown","source":"This model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.\n\nThe model is trained on the concatenation of Wikipedia in 104 different languages listed here. The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base). On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base","metadata":{"vscode":{"languageId":"plaintext"}}},{"cell_type":"markdown","source":"**Importar Librerias**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_nlp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-17T22:02:52.430122Z","iopub.execute_input":"2024-09-17T22:02:52.431081Z","iopub.status.idle":"2024-09-17T22:02:52.436878Z","shell.execute_reply.started":"2024-09-17T22:02:52.431036Z","shell.execute_reply":"2024-09-17T22:02:52.435906Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.16.1\nKerasNLP version: 0.14.4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"__Conf__","metadata":{"execution":{"iopub.status.busy":"2024-09-05T22:47:04.655602Z","iopub.execute_input":"2024-09-05T22:47:04.656333Z","iopub.status.idle":"2024-09-05T22:47:05.530781Z","shell.execute_reply.started":"2024-09-05T22:47:04.65629Z","shell.execute_reply":"2024-09-05T22:47:05.529252Z"}}},{"cell_type":"code","source":"try:\n    # detect and init the TPU\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    strategy = tf.distribute.TPUStrategy(resolver)\n    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()  # default strategy if no TPU available","metadata":{"execution":{"iopub.status.busy":"2024-09-17T22:02:52.438563Z","iopub.execute_input":"2024-09-17T22:02:52.439286Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nWARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"free(): corrupted unsorted chunks\nhttps://symbolize.stripped_domain/r/?trace=78ac5bd76e2c,78ac5bd2804f,5796fbbbe3bf,5796fbbbe3bf&map= \n*** SIGABRT received by PID 13 (TID 843) on cpu 88 from PID 13; stack trace: ***\nPC: @     0x78ac5bd76e2c  (unknown)  (unknown)\n    @     0x78ab69290387        928  (unknown)\n    @     0x78ac5bd28050       9808  (unknown)\n    @     0x5796fbbbe3c0  (unknown)  (unknown)\n    @     0x5796fbbbe3c0  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=78ac5bd76e2c,78ab69290386,78ac5bd2804f,5796fbbbe3bf,5796fbbbe3bf&map= \nE0917 22:02:54.792257     843 coredump_hook.cc:442] RAW: Remote crash data gathering hook invoked.\nE0917 22:02:54.792270     843 client.cc:269] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\nE0917 22:02:54.792273     843 coredump_hook.cc:537] RAW: Sending fingerprint to remote end.\nE0917 22:02:54.792300     843 coredump_hook.cc:546] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\nE0917 22:02:54.792304     843 coredump_hook.cc:598] RAW: Dumping core locally.\n","output_type":"stream"}]},{"cell_type":"code","source":"RESULT_DICT = {\n    0 : \"entailment\",\n    1 : \"neutral\",\n    2 : \"contradiction\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_pair_of_sentence(x):\n    print( \"Premise : \" + x['premise'])\n    print( \"Hypothesis: \" + x['hypothesis'])\n    print( \"Language: \" + x['language'])\n    print( \"Label: \" + str(x['label']))\n    print()\n\ndf_train.head(5).apply(lambda x : display_pair_of_sentence(x), axis=1)\n\ndf_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"premise_length\"] = df_train[\"premise\"].apply(lambda x : len(x))\ndf_train[\"hypothesis_length\"] = df_train[\"hypothesis\"].apply(lambda x : len(x))\ndf_train[[\"hypothesis_length\", \"premise_length\"]].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-09-12T22:35:23.362848Z","iopub.execute_input":"2024-09-12T22:35:23.363252Z","iopub.status.idle":"2024-09-12T22:35:23.367303Z","shell.execute_reply.started":"2024-09-12T22:35:23.363217Z","shell.execute_reply":"2024-09-12T22:35:23.366537Z"}}},{"cell_type":"code","source":"df_train = df_train[['premise', 'hypothesis', 'label']]\ndf_test = df_test[['premise', 'hypothesis']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train model","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name ='joeddav/xlm-roberta-large-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenize","metadata":{}},{"cell_type":"code","source":"def encode_premise_sentence(s):\n    tokens=[]\n    tokens.append('[CLS]')\n    tokens+=list(tokenizer.tokenize(s))\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_hypo_sentence(s):\n    tokens=[]\n    tokens.append('[sep]')\n    tokens+=list(tokenizer.tokenize(s))\n    tokens.append('[sep]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_hypo_sentence(\"jsalkgfad\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_premise_sentence(\"jsalkgfad\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized=[]\nfor i in range(len(df_train)):\n    pre=encode_premise_sentence(df_train['premise'][i])\n    hyp=encode_hypo_sentence(df_train['hypothesis'][i])\n    tokenized.append(pre+hyp)\ndf_train['tokenized']=tokenized\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Attention Mask and Token Type ID","metadata":{}},{"cell_type":"code","source":"mask=[]\nfor i in range(len(df_train)):\n    padded_seq=tokenizer(df_train['premise'][i],df_train['hypothesis'][i], padding=True,add_special_tokens = True)\n    mask.append(padded_seq)\ndf_train['masked'] = mask\ndf_train.head(5)\n# print(mask[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train Model","metadata":{}},{"cell_type":"code","source":"max_len=237\ndef build_model():\n    bert_encoder = TFAutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    embedding = bert_encoder([input_word_ids, input_mask])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def input_convert(data):\n    inputs={\n        'input_word_ids':[],\n        'input_mask':[]\n    }\n    for each in data:\n        inputs['input_word_ids'].append(each['input_ids'])\n        inputs['input_mask'].append(each['attention_mask'])\n        \n    inputs['input_word_ids']= tf.ragged.constant( inputs['input_word_ids']).to_tensor()\n    inputs['input_mask']= tf.ragged.constant( inputs['input_mask']).to_tensor()\n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input=input_convert(df_train['masked'].values)\nfor key in train_input.keys():\n    train_input[key] = train_input[key][:,:max_len]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFXLMRobertaModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Forward pass through the model\n    # Ensure inputs are handled correctly\n    encoder_outputs = bert_encoder(\n        input_ids=input_word_ids,\n        attention_mask=input_mask\n    )\n    embedding = encoder_outputs.last_hidden_state  # Use last_hidden_state for embeddings\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Use tf.function to convert Keras tensors to TensorFlow tensors\n    @tf.function\n    def get_model_outputs(input_ids, attention_mask):\n        return bert_encoder(input_ids=input_ids, attention_mask=attention_mask)\n\n    # Forward pass through the model\n    encoder_outputs = get_model_outputs(input_word_ids, input_mask)\n    embedding = encoder_outputs.last_hidden_state  # Use last_hidden_state for embeddings\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import tensorflow as tf\nfrom transformers import TFXLMRobertaModel\n\ndef build_model():\n    max_len = 100  # Adjust this as needed\n\n    # Define input layers using tf.keras.Input\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    # Instantiate the transformer model\n    bert_encoder = TFXLMRobertaModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n    # Define a function to call the model\n    def call_transformer_model(input_ids, attention_mask):\n        # Ensure the inputs are TensorFlow tensors\n        return bert_encoder(input_ids=input_ids, attention_mask=attention_mask)\n    \n    # Call the model\n    encoder_outputs = call_transformer_model(input_word_ids, input_mask)\n    embedding = encoder_outputs.last_hidden_state  # Get the last hidden state\n\n    # Define the output layer\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])  # Using the first token (CLS token) for classification\n\n    # Build and compile the model\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Adjust learning rate as needed\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of preparing data\ntrain_input = {\n    'input_word_ids': tf.convert_to_tensor(train_enc['input_ids'], dtype=tf.int32),\n    'input_mask': tf.convert_to_tensor(train_enc['attention_mask'], dtype=tf.int32)\n}\n\n# Example of model training\nmodel = build_model()\nmodel.summary()\nmodel.fit(train_input, df_train['label'].values, epochs=5, verbose=1, batch_size=128, validation_split=0.1, callbacks=[early_stop])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    model.fit(train_input, df_train['label'].values, epochs=5, verbose=1, batch_size=128, validation_split=0.1, callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prediction","metadata":{}},{"cell_type":"code","source":"mask=[]\nfor i in range(len(df_test)):\n    padded_seq=tokenizer(df_test['premise'][i],df_test['hypothesis'][i],\n                        padding=True,add_special_tokens =True)\n    mask.append(padded_seq)\ndf_test['masked']=mask\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=[np.argmax(i) for i in model.predict(test_input)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALIDATION_SPLIT = 0.2\nTRAIN_SIZE = int(df_train.shape[0]*(1-VALIDATION_SPLIT))\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_labels(x, y):\n    return (x[0], x[1]), y\n\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        (\n            df_train[['premise','hypothesis']].values,\n            keras.utils.to_categorical(df_train['label'], num_classes=3)\n        )\n    )\n)\n\ntrain_dataset = training_dataset.take(TRAIN_SIZE)\nval_dataset = training_dataset.skip(TRAIN_SIZE)\n\n# Apply the preprocessor to every sample of train, val and test data using `map()`.\n# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n# https://www.tensorflow.org/guide/data_performance for details.\n\ntrain_preprocessed = train_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\nval_preprocessed = val_dataset.map(split_labels, tf.data.AUTOTUNE).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a BERT model.\nwith strategy.scope():\n    classifier = keras_nlp.models.DistilBertClassifier.from_preset(\"distil_bert_base_multi\", num_classes=3)\n\n    # in distributed training, the recommendation is to scale batch size and learning rate with the numer of workers.\n    classifier.compile(optimizer=keras.optimizers.Adam(1e-5*strategy.num_replicas_in_sync),\n                       loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                       metrics=['accuracy'])\n    \n    classifier.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Fine tunning distil bert__","metadata":{}},{"cell_type":"code","source":"EPOCHS=10\nhistory = classifier.fit(train_preprocessed,\n                         epochs=EPOCHS,\n                         validation_data=val_preprocessed\n                        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}